---
title: "airborne eDNA - COI"
author: "Fabian Roger"
date: "2/4/2020"
output: html_document
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```


```{r, message = FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(here)
library(ggplot2)
library(dada2)
library(knitr)
library(ShortRead)
library(DECIPHER)
library(future.apply) # for parallel processing
library(furrr)
library(rgbif)
library(ggtree)
library(ape)

```

This script follows the [DADA2 Pipeline Tutorial (1.12)](https://benjjneb.github.io/dada2/tutorial.html) 
The original article can be found [here](http://rdcu.be/ipGh)

The script was written with DADA2 version 1.14.0


```{r}
data.frame(`Sequencing centre` = "NGI",
           Project = "P14711, Y.Clough_19_01",  
           `data delivery` = "2020-01-31",
           Samples = "64 (2 * 32)",
           `Sequencing amount` = "?",
           `Sequencing type`  = "1 lane on MiSeq 2x300 bp (V3)",
           `Amplicon type` = "COI (BF3-BR2)", 
           `Amplicon type (II)` = "16S (Chiar16SF - Chiar16SR)") %>% 
  kable()
```

```{r}
#COI
data.frame(`.` = c( "sequence","direction","length"),
           BF3 = c( "CCHGAYATRGCHTTYCCHCG", "foward", nchar("CCHGAYATRGCHTTYCCHCG")),
           BR2 = c( "TCDGGRTGNCCRAARAAYCA", "reverse", nchar("TCDGGRTGNCCRAARAAYCA"))) %>% 
  kable(caption = "primer sequences")

```

```{r}
#16S
data.frame(`.` = c( "sequence","direction","length"),
           BF3 = c( "TARTYCAACATCGRGGTC", "foward", nchar("TARTYCAACATCGRGGTC")),
           BR2 = c( "CYGTRCDAAGGTAGCATA", "reverse", nchar("CYGTRCDAAGGTAGCATA"))) %>% 
  kable(caption = "primer sequences")

```

```{r}
#Illumina adapter
#foward: "ACACTCTTTCCCTACACGACGCTCTTCCGATCT"
#reverse: "AGACGTGTGCTCTTCCGATCT"
```

load sample information
```{r}
sampleInfo <- read_tsv(here("Data", "Sequencing_data", "P14711", "00-Reports", "Y.Clough_19_01_sample_info.txt"))

sampleInfo <- 
  sampleInfo %>% 
  mutate(Barcode = gsub("(.+)_.+", "\\1", `User ID`),
         ID = gsub(".+_(.+)", "\\1", `User ID`),
         Station = gsub("([A-Z]+)[0-9]{0,2}", "\\1", `ID`))
  
```


create new folder for COI sequencing analysis
create new folder for raw fastq sequencing data

```{r, eval = FALSE}
dir.create(here("Data", "Sequencing_data", "COI"))
dir.create(here("Data", "Sequencing_data", "COI", "raw"))
```

find all fastq files in raw data from NGI (should be 2*64 = 128)
copy all COI fastQ files to COI/raw

```{r, eval = FALSE}
#list all files including in subfolder, give full path back
files <- list.files(here("Data", "Sequencing_data", "P14711"),
           full.names = TRUE, recursive = TRUE)

#grep for compressed fastq files
files_fastq <- files[grep(".+\\.fastq\\.gz", files)]

#check if we find the expected number of files
length(files_fastq) == 128

#names of COI samples
COI_samples <- 
sampleInfo %>% 
  filter(Barcode == "COI") %>% 
  pull(`NGI ID`)

files_COI <- files_fastq[grep(paste0(COI_samples, collapse = "|"), files_fastq)]

length(files_COI) == 64

file.copy(files_COI, here("Data", "Sequencing_data", "COI", "raw"))
```


define path variable that points to extracted, unmerged, sequencing reads

```{r, "path"}
path <- here("Data", "Sequencing_data", "COI", "raw")

# extract parent path
ParentPath <- here("Data", "Sequencing_data", "COI")

#path to plots
plotpath <- here("Data", "Sequencing_data", "COI", "plots")

#path to trimmed reads
trimmedpath <- here("Data", "Sequencing_data", "COI", "trimmed")

# make directory for trimmed files
dir.create( path =  trimmedpath)

# make directory for plots
dir.create( path =  plotpath)

#path to filtered reads
filterpath <- here("Data", "Sequencing_data", "COI", "DADA_filtered")

# make directory for filtered files
dir.create( path =  filterpath)

# list files
fns <- list.files(path)

data.frame(forward = fns[grepl("R1", fns)],
           reverse = fns[grepl("R2", fns)]) %>% 
  kable(caption = "Files in directory")
  
```

#remove primers
we check if the sequences still have primers / adapter sequences attached that need to be removed
For that we read in the first file and search for the primer sequence (and it's reverse complement) 

```{r}
# read in first sample. These are foward reads
Sample1F <- readFastq(paste(path, fns[1], sep = "/"))
Sample1R <- readFastq(paste(path, fns[2], sep = "/"))

# have a look at first 6 Sequences. 
sread(head(Sample1F))

# check for foward primer in first 10 000 sequences
# COI foward primer ["CCHGAYATRGCHTTYCCHCG"]

FPM <- vmatchPattern("CCHGAYATRGCHTTYCCHCG", sread(Sample1F)[1:10000], fixed=FALSE) 

start(FPM) %>% unlist %>% table
width(FPM) %>% unlist %>% table


# check for reverse primer in first 10 000 Seqs
# COI foward primer ["TCDGGRTGNCCRAARAAYCA"]
RPM <- vmatchPattern("TCDGGRTGNCCRAARAAYCA", sread(Sample1R)[1:10000], fixed=FALSE) 

start(RPM) %>% unlist %>% table
width(RPM) %>% unlist %>% table





```

We can see that both foward and reverse primers are still on (which makes sense as the reads are 301 bp long and thus must include primers). We need to remove them before we proceed.

We will use [cutadapt](https://cutadapt.readthedocs.io/en/stable/installation.html) to remove the forward and reverse primers from the paired reads.

To install cutadapt follow the installation instruction in the link. (you might need to install it in a virtual environment on macs)

cutadapt is executed from the command line but we can call it from within r

note: here I installed cutadapt in a virtual environment called 'my_env' and which is located under `/Users/fabian/my_env`
I first need to activate the virtual environment then I can use cutadapt. 

If you get a `command not found` error you either don't have cutadapt installed or it's not found (you need to activate your vitenv or put cutadapt in your PATH)

We trim the primers following the instructions [here](https://cutadapt.readthedocs.io/en/stable/recipes.html#trimming-amplicon-primers-from-both-ends-of-paired-end-reads)

Forward primer:

```{r}
Fwrd <- "CCHGAYATRGCHTTYCCHCG"
```

reverse complement Forward primer:

```{r}
RC_Fwrd <- reverseComplement(DNAString(Fwrd))
RC_Fwrd
```

Reverse Primer:

```{r}
Rev <- "TCDGGRTGNCCRAARAAYCA"
```

reverse complement Reverse Primer

```{r}
RC_Rev <- reverseComplement(DNAString(Rev))
RC_Rev
```


```{r}
forward_raw <- fns[grepl("R1", fns)] 
reverse_raw <- fns[grepl("R2", fns)]

#minimum length for sequences after trimming (shorter sequences are discarded)
minlength <- 100

#number of cores to use 
Cores <- 4


for (i in seq_along(forward_raw)) {

temp <- 
system(
paste("source /Users/fabian/my_env/bin/activate
       cutadapt -a ", Fwrd, "...", RC_Rev,
      " -A ", Rev, "...", RC_Fwrd,
      " --discard-untrimmed",
      " --minimum-length=", minlength, #minimum length for sequences after trimming
      " --cores=", Cores, # Number of cores to use
      " -o ", paste(trimmedpath, "/trimmed_", forward_raw[i], sep = ""),
      " -p ", paste(trimmedpath, "/trimmed_", reverse_raw[i], sep = ""),
      paste(" ", path, "/", forward_raw[i], sep = ""),
      paste(" ", path, "/", reverse_raw[i], sep = ""),
      sep = ""
      ),

intern = TRUE
)

rm(temp)
}


```

Now we read in the file names for all the trimmed fastq files and do a little string manipulation to get lists of the forward and reverse fastq files in matched order:

```{r, "sort fwrd and rev", cache=TRUE}

fnFs <- sort(list.files(trimmedpath, pattern="_R1_001.fastq.gz", full.names = TRUE)) # Just the forward read files
fnRs <- sort(list.files(trimmedpath, pattern="_R2_001.fastq.gz", full.names = TRUE)) # Just the reverse read files

# Get sample names from the first part of the forward read filenames
sample.names <- sapply(fnFs, function(x) gsub(".+?([A-Z,0-9]+_[0-9]+).+", "\\1", x), USE.NAMES = FALSE)

```

#quality profiles

Visualize the quality profile of the forward reads

```{r, "plot quality of reads", cache=TRUE}
# plot one foward and one reverse read in the rapport
plotQualityProfile(fnFs[[1]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "foward", sep = " - " ))

plotQualityProfile(fnRs[[1]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "reverse", sep = " - "))

plotlistF <- list()

for( i in  seq_along(fnFs)) {
  plot <-  plotQualityProfile(fnFs[[i]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[i], "foward", sep = " - " ))
  plotlistF[[i]] <- plot
}


plotlistR <- list()

for( i in  seq_along(fnRs)) {
  plot <-  plotQualityProfile(fnRs[[i]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[i], "reverse", sep = " - "))
  plotlistR[[i]] <- plot
}


# interleave the two lists:

plotlist <- c(plotlistF, plotlistR)[order(c(seq_along(plotlistF), seq_along(plotlistR)))]

ggsave(paste(plotpath, "quality_plots.pdf", sep = "/"), marrangeGrob(grobs = plotlist, nrow=2, ncol=1), height = 10, width = 8)


```
The quality of the sequences looks very good as the median Q-score stays over 30 for the whole expected length (280 bp, 300 bp - 20bp primers)

The reverse read looks slightly worth (which is expected) but still good. 

We used the **BF3** - **BR2** Primers (Elbrecht et al, 2019). For insects, the primer should give us a fragment of 418 bp length (without primers). 

Both primers are 20 bp long, wherefore the expected length of both the foward an reverse reads is 280 bp. 

Thus we have an overlap of `(280*2-418)/2` = 71 bp

We cut the reverse read to 260 bp, keeping an overlapp of 51 bp. We do not trim the foward reads. 

The filtering parameters we’ll use are standard (however we allow for more expected errors as we only cut few bases and DADA is [robust to low quality sequences](https://twitter.com/bejcal/status/771010634074820608) )

+ maxN=0 (DADA2 requires no Ns)
+ truncQ=25 
+ maxEE=c(2,4)

The `maxEE` parameter sets the maximum number of “expected errors” allowed in a read. We allow for more expected errors in the reverse read as the quality is slightly lower.

We use the `fastqPairedFilter` function to jointly filter the forward and reverse reads.

#filter and trim
```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(filterpath, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filterpath, paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


```{r}

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(280,260),
                     maxN=0,
                     maxEE=c(2,4),
                     truncQ=2,
                     rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE) 

head(out)
```

#learn errors
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
```


```{r, cache=TRUE}
plotErrors(errF, nominalQ=TRUE)
ggsave(paste(plotpath, "errorPlot_F.pdf", sep = "/"))

plotErrors(errR, nominalQ=TRUE)
ggsave(paste(plotpath, "errorPlot_R.pdf", sep = "/"))
```

#denoise
```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```

```{r}
dadaFs[1]
```

#merge
Merge paired reads
Spurious sequence variants are further reduced by merging overlapping reads. 

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

head(mergers[[1]])

```

Construct sequence table:

```{r, "Seqeunce table"}

seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```


#remove chimera


```{r, "Chimera removal", cache=TRUE}

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

```

percentage of Chimeric sequences: `r (1-(sum(seqtab.nochim)/sum(seqtab)))*100`

percentage of Chimeric ASVs: `r (1-(ncol(seqtab.nochim)/ncol(seqtab)))*100`


# export ASV table

```{r}
write.table(seqtab.nochim, here("Data", "COI_ASV_table.text"))
```

check sequence length

```{r}
data.frame(table(nchar(colnames(seqtab.nochim)))) %>% 
  ggplot( aes(x = Var1, y = Freq))+
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = -90))+
  labs(x = "length in bp", title = "histogramm of merged sequence lenghts")

#reads
tibble(seq_length = nchar(colnames(seqtab.nochim)),
           seq_abund = colSums(seqtab.nochim)) %>% 
  group_by(seq_length) %>% 
  summarise(len_abund = sum(seq_abund)) %>% 
  ungroup() %>% 
  mutate(exp = if_else(seq_length %in% c(412:424), "exp", "unexp")) %>% #allowing for some variation aroound teh expected read length of 418
  group_by(exp) %>% 
  summarise(len_abund = sum(len_abund)) %>% 
  mutate(prct = round(len_abund / sum(len_abund) * 100,2))
 

```
# length filtering

```{r}
seqtab.nochim_rlen <- seqtab.nochim[, nchar(colnames(seqtab.nochim)) %in% c(412:424)]
```


#track reads

how much reads lost during cutadapt?

```{r}
raw_seq <- list.files(path, full.names = TRUE)
raw_seq <- raw_seq[grepl("R1", raw_seq)]

raw_reads <- sapply(raw_seq, function(x) system(paste("zcat < ", x, " | echo $((`wc -l`/4))", sep = ""), intern = TRUE))

cutadapt <- 
  tibble(sample = sample.names, 
         raw = as.numeric(raw_reads))
```


read in sample metadata
```{r}
sampleInfo <- read_tsv(here::here("Data", "Sequencing_data", "P14711", "00-Reports", "Y.Clough_19_01_sample_info.txt"))

# Read in file with sample info, from DADA2/NGI(?)

sampleInfo <- 
  sampleInfo %>% 
  mutate(Barcode = gsub("(.+)_.+", "\\1", `User ID`),
         ID = gsub(".+_(.+)", "\\1", `User ID`),
         Station = gsub("([A-Z]+)[0-9]{0,2}", "\\1", `ID`)) %>% 
  dplyr::rename(sample = "NGI ID")

sampleInfo <- filter(sampleInfo, Barcode == "COI")
```



```{r}
# summary of read numbers through DADA2 workflow
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names



track %>% 
  as.data.frame %>% 
  tibble::rownames_to_column(var="sample") %>% 
  mutate("len_filtered" = rowSums(seqtab.nochim_rlen)) %>% 
  left_join(cutadapt, .) %>% 
  mutate(across(!one_of("sample"),  function(x) x/raw*100)) %>% 
  pivot_longer(!one_of("sample"), names_to = "step", values_to = "reads") %>% 
  mutate(step = factor(step, levels = c("raw", "input", "filtered", 
                                        "denoised", "merged", 
                                        "tabled", "nonchim", "len_filtered"))) %>%
  left_join(select(sampleInfo, sample, Station)) %>% 
  ggplot(aes(x=step, y=reads, group = sample, colour = Station))+
  geom_line(position = position_jitter(width = 0.1), alpha = 0.8)+
  geom_point(position = position_jitter(width = 0.1),
             alpha = 0.3, shape = 21)+
  geom_violin(fill = NA, aes(group = step))+
  stat_summary(geom="line", fun.y="median", aes(group = 1))+
  scale_y_continuous(breaks = seq(0,100,10), limits = c(0,102))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = -30, hjust = 0))+
  scale_colour_brewer(palette = "Set1")

ggsave(paste(plotpath, "sequence_filtering.pdf", sep = "/"))
```

#export ASVs table

```{r}
write.table(seqtab.nochim_rlen, here("Data", "COI_ASV_table_rl.text"))
saveRDS(seqtab.nochim_rlen, here("Data", "seqtab.nochim_rl.RDS"))
#seqtab.nochim_rlen <- readRDS(here("Data", "seqtab.nochim_rl.RDS"))
```


export ASVs to Fasta file 

```{r}
seqs <- DNAStringSet(colnames(seqtab.nochim_rlen))

names(seqs) <- paste("seq", 
                             formatC(1:length(seqs), flag = "0",
                                     width = nchar(length(seqs))),
                             sep = "_")

writeXStringSet(seqs, here("Data", "COI_ASVs.fasta"), format = "fasta")

seqs <- readDNAStringSet(here("Data", "COI_ASVs.fasta"))
```


```{r}
colnames(seqtab.nochim_rlen) <- names(seqs)
```

test clustering threshold
```{r}

RES <- tibble(ID = numeric(),
       n_cluster = numeric())

for (i in seq(0.9, 1, 0.005)){
  
  system(paste(
    "/Applications/vsearch/bin/vsearch --cluster_fast ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/COI_ASVs.fasta --centroids ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/test.fasta --id ", i))
  
  L <- system("grep -c '^>' ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/test.fasta", intern = T) 
  
  RES_temp <- tibble(ID = i,
                     n_cluster = as.numeric(L))
  
  RES <- rbind(RES, RES_temp)
  
}

ggplot(RES, aes(x = ID, y = n_cluster))+
  geom_point()

```


#cluster AVSs to 1% similarity

```{r}
system(
    "/Applications/vsearch/bin/vsearch --cluster_fast ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/COI_ASVs.fasta --centroids ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/COI_clustered_99.fasta --id 0.99 --uc ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/COI_99_cluster.txt")

```

The COI_99_cluster.txt file tells us which sequences have been clustered

column 09 gives the original sequence name
column 10 gives the centroid sequence to which it has been clustered
column 01 tells us if the sequence is itself a centroid sequence ("S") or clustered to a centroid ("H")
column 04 gives the % identity with the centroid (here between 100% and 99%)

```{r}

Cluster_ID <- read_delim(here("Data", "COI_99_cluster.txt"), delim = "\t", col_names = FALSE)

#filter only sequences that have been clusterd
Cluster_ID <- 
Cluster_ID %>% filter(X1 != "C") %>% 
  dplyr::rename(seq = X9, Type = X1, match = X10, pident = X4) %>% 
  select(seq, Type, match, pident) %>% 
  filter(Type == "H" ) %>% 
  arrange(match)

#split by cluster centroid
Cluster_list <- split(Cluster_ID, Cluster_ID$match)#

#rename sequences in ASV table
colnames(seqtab.nochim_rlen) <- paste("seq", 
                             formatC(1:ncol(seqtab.nochim_rlen), flag = "0",
                                     width = nchar(ncol(seqtab.nochim_rlen))),
                             sep = "_")

#sum abundance of clustered sequences
seqtab.nochim_list <- 
  lapply(Cluster_list, function(x) {
  S_seq <- unique(x$match)
  C_seqs <- x$seq
  seqtab.nochim_rlen[,S_seq] <- rowSums(seqtab.nochim_rlen[,c(S_seq,C_seqs)])
})

#create new ASV table with only the centroid sequences
seqtab.nochim_C <- seqtab.nochim_rlen
seqtab.nochim_C[,names(seqtab.nochim_list)] <- t(bind_rows(seqtab.nochim_list))
seqtab.nochim_C <- seqtab.nochim_C[, which(! colnames(seqtab.nochim_C) %in% Cluster_ID$seq)] 

#export ASV table
write.table(seqtab.nochim_C, here("Data", "COI_ASV_table_99.text"))
#seqtab.nochim_C <- read.table(here::here("Data", "COI_ASV_table_99.text"))
```

#Sintax

```{bash}

/Applications/vsearch/bin/vsearch --sintax /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/COI_clustered_99.fasta --db /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/MIDORI_UNIQ_SP_GB242_CO1_SINTAX.fasta --sintax_cutoff 0.1 --tabbedout /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/COI_midori_tax.txt

```

```{r}
Sintax_taxa <- read_lines(here("Data", "COI_midori_tax.txt"))

#get table with all assignments (even the unlikely ones)
Sintax_taxa_table_full <- 
tibble(seq = gsub("(seq_\\d+).+", "\\1",Sintax_taxa),
       taxonomy = gsub("seq_\\d+\\\tk:(.+)\\\t\\+\\\tk:.+", "\\1", Sintax_taxa)) %>% 
  mutate(taxonomy = case_when(grepl("\t", taxonomy, fixed = T) ~ NA_character_,
                              TRUE ~ taxonomy)) %>% 
  mutate(taxonomy = gsub("_.+?:", "\t", taxonomy)) %>%
  mutate(taxonomy = gsub("_\\d+.+$", "", taxonomy)) %>%
  separate(taxonomy, into = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"), sep = "\t") %>% 
  arrange(seq)

#get table with probabilities for each assignment
Sintax_taxa_table_prob <- 
  tibble(seq = gsub("(seq_\\d+).+", "\\1",Sintax_taxa),
       taxonomy = gsub("seq_\\d+\\\tk:(.+)\\\t\\+\\\tk:.+", "\\1", Sintax_taxa)) %>% 
  mutate(taxonomy = case_when(grepl("\t", taxonomy, fixed = T) ~ NA_character_,
                              TRUE ~ taxonomy)) %>% 
  mutate(taxonomy = gsub(".+?(\\(\\d\\.\\d\\d\\)).+?", "\\1", taxonomy)) %>%
  mutate(taxonomy = gsub("s:.+?(\\(\\d\\.\\d\\d\\))$", "\\1", taxonomy)) %>%
  mutate(taxonomy = gsub("\\)\\(", "\t", taxonomy)) %>% 
  mutate(taxonomy = gsub("\\)|\\(", "", taxonomy)) %>% 
  separate(taxonomy, into = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"), sep = "\t") %>% 
  arrange(seq) %>% 
  mutate(across(2:8, as.numeric))
```


##rgbif

harmonizing taxonomy with gbif backbone for comparability with 16S data

```{r}

plan(multisession)

gbif_specnames <- 
future_map(1:nrow(Sintax_taxa_table_full), function(x) {
  name_backbone(name = Sintax_taxa_table_full$Species[x], 
                rank = "species", 
                kingdom = Sintax_taxa_table_full$Kingdom[x],
                phylum = Sintax_taxa_table_full$Phylum[x],
                class = Sintax_taxa_table_full$Class[x],
                order = Sintax_taxa_table_full$Order[x],
                family = Sintax_taxa_table_full$Family[x])
},
.progress = TRUE) %>% bind_rows()


Sintax_taxa_table_full <-  
  gbif_specnames %>% 
  select(kingdom, phylum, class, order, family, genus, species) %>% 
  mutate(seq = Sintax_taxa_table_full$seq) %>% 
  relocate(seq)

```


export

```{r}
write_tsv(Sintax_taxa_table_full, here("Data", "COI_99_Sintax_tax.txt"))
write_tsv(Sintax_taxa_table_prob, here("Data", "COI_99_Sintax_prob.txt"))
```



#boldigger

The following is some additional code to do an alternative sequence assignment with bolddigeer. In the end we decided not to use it for the article. 


vsearch seems to introduce line breaks into the fasta file that boldiger cannot deal with, we import it and export a copy as boldiger seems to work on the raw data file

```{r}
COI_99 <- readDNAStringSet(here("Data", "COI_clustered_99.fasta"))

COI_99 <- COI_99[sort(names(COI_99))]

writeXStringSet(COI_99, here("Data", "COI_clustered_99_boldiger.fasta"))
# -->  manually edit out linebreaks
```

(copy past code in shell, doesn't run from script)
```{bash}
mkdir /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/boldiger

boldigger-cline ie_coi "fabianroger" "U3z6&%uCE6co" /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/COI_clustered_99_boldiger.fasta /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/boldiger

boldigger-cline add_metadata /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/boldiger/BOLDResults_COI_clustered_99_boldiger.xlsx

boldigger-cline digger_hit /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/boldiger/BOLDResults_COI_clustered_99_boldiger.xlsx
```


```{r}
BD_hit =  readxl::read_xlsx(here("Data", "boldiger", "BOLDResults_COI_clustered_99_boldiger.xlsx"), sheet = "BOLDigger hit")

COI_bold <-
BD_hit %>% 
  mutate(ID = gsub(">", "", ID)) %>% 
  dplyr::rename(seq = ID) %>% 
  mutate(Species = paste(Genus, Species, sep = " ")) %>% 
  mutate(Species = ifelse(Species == "NA NA", NA_character_, Species)) 

```


harmonizing taxonomy with gbif backbone for comparability with 16S data

```{r}

plan(multisession)

COI_bold_gbif <- 
future_map(1:nrow(COI_bold), function(x) {
  name_backbone(name = COI_bold$Species[x], 
                rank = "species", 
                phylum = COI_bold$Phylum[x],
                class = COI_bold$Class[x],
                order = COI_bold$Order[x],
                family = COI_bold$Family[x],
                genus = COI_bold$Genus[x])
},
.progress = TRUE) %>% bind_rows()


COI_bold <-  
  COI_bold_gbif %>% 
  select(kingdom, phylum, class, order, family, genus, species) %>% 
  mutate(seq = COI_bold$seq, 
         Similarity = COI_bold$Similarity) %>% 
  relocate(seq) 
  
write_tsv(COI_bold, here("Data", "boldiger", "COI_boldiger_tax.txt"))
```


#distance

Align sequences
```{r}
#import
COI_seq <- readDNAStringSet(here("Data","COI_clustered_99.fasta"))

#align
COI_aligned <- AlignSeqs(COI_seq, processors = 4)

#stagger alignment
COI_staggered <- StaggerAlignment(COI_aligned)

#caluclate distance matrix
COI_dist <- DistanceMatrix(COI_staggered)

```

```{r}
Sintax_taxa_table_full <- read_tsv(here("Data", "COI_99_Sintax_tax.txt"))
Sintax_taxa_table_prob <- read_tsv(here("Data", "COI_99_Sintax_prob.txt"))

sintax_06 <- Sintax_taxa_table_full
sintax_06[Sintax_taxa_table_prob < 0.6] <- NA 

sintax_06 %>% filter(is.na(Kingdom))

COI_dist %>% 
  colSums() %>% 
  data.frame(dist = .) %>% 
  tibble::rownames_to_column("seq") %>% 
  arrange(desc(dist)) %>% 
  mutate(rank = 1:n()) %>% 
  #dplyr::slice(1:1000) %>% 
  left_join(sintax_08) %>% 
  #filter(is.na(Phylum)) %>% 
  ggplot(aes(x = dist, fill = Kingdom))+
  geom_histogram()
```

#phylogeny 

To complement the erroneous sequence filtering above, we build a phylogeny to detect aberrant sequences. For this we Align the Sequences with Decipher

```{r}
#import
COI_seq <- readDNAStringSet(here("Data","COI_clustered_99.fasta"))

msa <- AlignSeqs(COI_seq, processors = 4)  
```
We also follow [the advice](https://www.bioconductor.org/packages/devel/bioc/vignettes/DECIPHER/inst/doc/ArtOfAlignmentInR.pdf) to Stagger the alignment for more accurate Tree building

>To mitigate the problem of false homologies, StaggerAlignment will automatically generate a staggered version of an existing alignment. Staggered alignments separate potentially non-homologous regions into separate columns of the alignment. The result is an alignment that is less visually appealing, but likely more accurate in a phylogenetic sense. As such, this is an important post-processing step whenever the alignment will be used to construct a phylogenetic tree


```{r}
msa_stag <- StaggerAlignment(msa, verbose = FALSE)

writeXStringSet(msa_stag, file= here::here("Data", "COI_aligned.fasta"))

msa_stag <- readDNAStringSet(here("Data", "COI_aligned.fasta"))
distmsa <- DistanceMatrix(msa_stag)
```

```{r}
Sintax_taxa_table_full <- read_tsv(here("Data", "COI_99_Sintax_tax.txt"))
Sintax_taxa_table_prob <- read_tsv(here("Data", "COI_99_Sintax_prob.txt"))

sintax_08 <- Sintax_taxa_table_full
sintax_08[Sintax_taxa_table_prob < 0.8] <- NA 

distSum <- 
as.matrix(distmsa) %>% 
  rowSums() %>% 
  data.frame(dist = .) %>% 
  tibble::rownames_to_column("seq") %>% 
  arrange(desc(dist)) %>% 
  mutate(rank = 1:n()) 

distSum %>% 
  left_join(sintax_08) %>% 
  #filter(is.na(Phylum)) %>% 
  ggplot(aes(x = dist, fill = kingdom))+
  geom_histogram()

```


We will use [FastTree](http://meta.microbesonline.org/fasttree/) to estimate an approximately-maximum-likelihood phylogenetic tree. It is orders of magnitudes faster than the corresponding R implementation phangorn. Also, because the downstream estimation of phylogenetic diversity requires an ultrametric tree, we use [PATHd8](http://www2.math.su.se/PATHd8/) for the phylogenetic dating. 

note that PATH8 produces a big file with lots of additional information that is not very relevant in our case. Therefore we grep the tree from the produced output file and save it separately. 

```{r}
system(paste("/Applications/FastTree -gtr -nt",
              here::here("Data", "COI_aligned.fasta"),
              ">",
              here::here("Data", "COI_FastTree.tre")))

TREE <- read.tree(here::here("Data", "COI_FastTree.tre"))

TREE <- root(TREE, "seq_3455") #confirmed Actionbacteria sequence, set as outgroup here
```

#plot tree 


```{r}

#TREE <- drop.tip(TREE, ASV_excl)

Tax_annot <- 
  Sintax_taxa_table_full %>% 
  mutate(Phylum = case_when(phylum %in% c("Arthropoda", "Ascomycota", 
                                          "Oomycota", "Chordata", "Basidiomycota",
                                          "Rhodophyta", "Mollusca", "Chordata") ~ phylum,
                            ! is.na(phylum) ~ "Other",
                            TRUE ~ NA_character_)) %>%
  select(seq, kingdom, phylum) %>% 
  left_join(Sintax_taxa_table_prob, by = "seq")
  

Tax_abund <-  
  colSums(seqtab.nochim_C) %>% 
    data.frame(abund = .) %>% 
    tibble::rownames_to_column(var = "seq") %>% 
    mutate(log_abund = log(abund+1, base = 10))

p_TREE <- ggtree(TREE)

p1 <- 
p_TREE+
  #geom_tiplab(align =TRUE, linesize = 0)+
  geom_facet(panel = "Phylum", data = Tax_annot, geom = geom_segment, 
               mapping=aes(x = 1, xend = 3, yend = y, colour = Phylum.x, group = 1))+
  scale_colour_brewer(palette = "Paired")+
  geom_facet(panel = "probability", data = Tax_annot,  geom = geom_tile,
               mapping=aes(x = 1, fill = Phylum.y))+
  scale_fill_viridis_c()+
  theme(legend.position = "bottom")

ggsave(here("Figures", "COI_Tree_annotated.pdf"), p1,  width = 30, height = 10)

plot(p1)  
```


