---
title: "airborne eDNA - 16S"
author: "Fabian Roger"
date: "2/23/2020"
output: html_document
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```


```{r, message = FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(here)
library(ggplot2)
library(gridExtra)
library(dada2)
library(knitr)
library(ShortRead)
library(DECIPHER)
library(viridis)
library(ape)
library(ggtree)
# library(vegan)

# library(phyloseq)
# 

# library(ape)
# library(viridis)
```

This script follows the [DADA2 Pipeline Tutorial (1.12)](https://benjjneb.github.io/dada2/tutorial.html) 
The original article can be found [here](http://rdcu.be/ipGh)

The script was written with DADA2 version 1.14.0


```{r}
data.frame(`Sequencing centre` = "NGI",
           Project = "P14711, Y.Clough_19_01",  
           `data delivery` = "2020-01-31",
           Samples = "64 (2 * 32)",
           `Sequencing amount` = "?",
           `Sequencing type`  = "1 lane on MiSeq 2x300 bp (V3)",
           `Amplicon type` = "COI (BF3-BR2)", 
           `Amplicon type (II)` = "16S (Chiar16SF - Chiar16SR)") %>% 
  kable()
```

```{r}
#COI
data.frame(`.` = c( "sequence","direction","length"),
           BF3 = c( "CCHGAYATRGCHTTYCCHCG", "foward", nchar("CCHGAYATRGCHTTYCCHCG")),
           BR2 = c( "TCDGGRTGNCCRAARAAYCA", "reverse", nchar("TCDGGRTGNCCRAARAAYCA"))) %>% 
  kable(caption = "primer sequences")

```

```{r}
#16S
data.frame(`.` = c( "sequence","direction","length"),
           BF3 = c( "TARTYCAACATCGRGGTC", "foward", nchar("TARTYCAACATCGRGGTC")),
           BR2 = c( "CYGTRCDAAGGTAGCATA", "reverse", nchar("CYGTRCDAAGGTAGCATA"))) %>% 
  kable(caption = "primer sequences")

```

```{r}
#Illumina adapter
#foward: "ACACTCTTTCCCTACACGACGCTCTTCCGATCT"
#reverse: "AGACGTGTGCTCTTCCGATCT"
```

load sample information
```{r}
sampleInfo <- read_tsv(here("Data", "Sequencing_data", "P14711", "00-Reports", "Y.Clough_19_01_sample_info.txt"))

sampleInfo <- 
  sampleInfo %>% 
  mutate(Barcode = gsub("(.+)_.+", "\\1", `User ID`),
         ID = gsub(".+_(.+)", "\\1", `User ID`),
         Station = gsub("([A-Z]+)[0-9]{0,2}", "\\1", `ID`))
  
```


create new folder for 16S sequencing analysis
create new folder for raw fastq sequencing data

```{r, eval = FALSE}
dir.create(here("Data", "Sequencing_data", "16S"))
dir.create(here("Data", "Sequencing_data", "16S", "raw"))
```

find all fastq files in raw data from NGI (should be 2*64 = 128)
copy all COI fastQ files to 16S/raw

```{r, eval = FALSE}
#list all files including in subfolder, give full path back
files <- list.files(here("Data", "Sequencing_data", "P14711"),
           full.names = TRUE, recursive = TRUE)

#grep for compressed fastq files
files_fastq <- files[grep(".+\\.fastq\\.gz", files)]

#check if we find the expected number of files
length(files_fastq) == 128

#names of 16S samples
samples_16S <- 
sampleInfo %>% 
  filter(Barcode == "16S") %>% 
  pull(`NGI ID`)

files_16S <- files_fastq[grep(paste0(samples_16S, collapse = "|"), files_fastq)]

length(files_16S) == 64

file.copy(files_16S, here("Data", "Sequencing_data", "16S", "raw"))
```


define path variable that points to extracted, unmerged, sequencing reads

```{r, "path"}
path <- here("Data", "Sequencing_data", "16S", "raw")

# extract parent path
ParentPath <- here("Data", "Sequencing_data", "16S")

#path to plots
plotpath <- here("Data", "Sequencing_data", "16S", "plots")

#path to trimmed reads
trimmedpath <- here("Data", "Sequencing_data", "16S", "trimmed")

# make directory for trimmed files
dir.create( path =  trimmedpath)

# make directory for plots
dir.create( path =  plotpath)

#path to filtered reads
filterpath <- here("Data", "Sequencing_data", "16S", "DADA_filtered")

# make directory for filtered files
dir.create( path =  filterpath)

# list files
fns <- list.files(path)

data.frame(forward = fns[grepl("R1", fns)],
           reverse = fns[grepl("R2", fns)]) %>% 
  kable(caption = "Files in directory")
  
```

we check if the sequences still have primers / adapter sequences attached that need to be removed
For that we read in the first file and search for the primer sequence (and it's reverse complement) 

```{r}
# read in first sample. These are foward reads
Sample1F <- readFastq(paste(path, fns[1], sep = "/"))
Sample1R <- readFastq(paste(path, fns[2], sep = "/"))

# have a look at first 6 Sequences. 
sread(head(Sample1F))

# check for foward primer in first 10 000 sequences
# 16S foward primer ["CCHGAYATRGCHTTYCCHCG"]

FPM <- vmatchPattern("TARTYCAACATCGRGGTC", sread(Sample1F)[1:10000], fixed=FALSE) 

start(FPM) %>% unlist %>% table
width(FPM) %>% unlist %>% table


# check for reverse primer in first 10 000 Seqs
# 16S reverse primer ["TCDGGRTGNCCRAARAAYCA"]
RPM <- vmatchPattern("CYGTRCDAAGGTAGCATA", sread(Sample1R)[1:10000], fixed=FALSE) 

start(RPM) %>% unlist %>% table
width(RPM) %>% unlist %>% table





```

We can see that both foward and reverse primers are still on (which makes sense as the reads are 301 bp long and thus must include primers). We need to remove them before we proceed.

We will use [cutadapt](https://cutadapt.readthedocs.io/en/stable/installation.html) to remove the forward and reverse primers from the paired reads.

To install cutadapt follow the installation instruction in the link. (you might need to install it in a virtual environment on macs)

cutadapt is executed from the command line but we can call it from within r

note: here I installed cutadapt in a virtual environment called 'my_env' and which is located under `/Users/fabian/my_env`
I first need to activate the virtual environment then I can use cutadapt. 

If you get a `command not found` error you either don't have cutadapt installed or it's not found (you need to activate your vitenv or put cutadapt in your PATH)

We trim the primers following the instructions [here](https://cutadapt.readthedocs.io/en/stable/recipes.html#trimming-amplicon-primers-from-both-ends-of-paired-end-reads)

Forward primer:

```{r}
Fwrd <- "TARTYCAACATCGRGGTC"
```

reverse complement Forward primer:

```{r}
RC_Fwrd <- reverseComplement(DNAString(Fwrd))
RC_Fwrd
```

Reverse Primer:

```{r}
Rev <- "CYGTRCDAAGGTAGCATA"
```

reverse complement Reverse Primer

```{r}
RC_Rev <- reverseComplement(DNAString(Rev))
RC_Rev
```


```{r}
forward_raw <- fns[grepl("R1", fns)] 
reverse_raw <- fns[grepl("R2", fns)]

#minimum length for sequences after trimming (shorter sequences are discarded)
minlength <- 100

#number of cores to use 
Cores <- 4


for (i in seq_along(forward_raw)) {

temp <- 
system(
paste("source /Users/fabian/my_env/bin/activate
       cutadapt -a ", Fwrd, "...", RC_Rev,
      " -A ", Rev, "...", RC_Fwrd,
      " --discard-untrimmed",
      " --minimum-length=", minlength, #minimum length for sequences after trimming
      " --cores=", Cores, # Number of cores to use
      " -o ", paste(trimmedpath, "/trimmed_", forward_raw[i], sep = ""),
      " -p ", paste(trimmedpath, "/trimmed_", reverse_raw[i], sep = ""),
      paste(" ", path, "/", forward_raw[i], sep = ""),
      paste(" ", path, "/", reverse_raw[i], sep = ""),
      sep = ""
      ),

intern = TRUE
)

rm(temp)
}


```

Now we read in the file names for all the trimmed fastq files and do a little string manipulation to get lists of the forward and reverse fastq files in matched order:

```{r, "sort fwrd and rev", cache=TRUE}

fnFs <- sort(list.files(trimmedpath, pattern="_R1_001.fastq.gz", full.names = TRUE)) # Just the forward read files
fnRs <- sort(list.files(trimmedpath, pattern="_R2_001.fastq.gz", full.names = TRUE)) # Just the reverse read files

# Get sample names from the first part of the forward read filenames
sample.names <- sapply(fnFs, function(x) gsub(".+?([A-Z,0-9]+_[0-9]+).+", "\\1", x), USE.NAMES = FALSE)

```



Visualize the quality profile of the forward reads

```{r, "plot quality of reads", cache=TRUE}
# plot one foward and one reverse read in the rapport
plotQualityProfile(fnFs[[1]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "foward", sep = " - " ))

plotQualityProfile(fnRs[[1]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "reverse", sep = " - "))

plotlistF <- list()

for( i in  seq_along(fnFs)) {
  plot <-  plotQualityProfile(fnFs[[i]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[i], "foward", sep = " - " ))
  plotlistF[[i]] <- plot
}


plotlistR <- list()

for( i in  seq_along(fnRs)) {
  plot <-  plotQualityProfile(fnRs[[i]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[i], "reverse", sep = " - "))
  plotlistR[[i]] <- plot
}


# interleave the two lists:

plotlist <- c(plotlistF, plotlistR)[order(c(seq_along(plotlistF), seq_along(plotlistR)))]

ggsave(paste(plotpath, "quality_plots.pdf", sep = "/"), marrangeGrob(grobs = plotlist, nrow=2, ncol=1), height = 10, width = 8)


```
The quality of the sequences looks very good as the median Q-score stays over 30 for the whole expected length (280 bp, 300 bp - 20bp primers)

The reverse read looks slightly worth (which is expected) but still good. 

We used the **Chiar16SF** - **Chiar16SR** Primers (Marquina et al 2018). For insects, the primer should give us a fragment of 348 bp length (without primers). 

Both primers are 18 bp long, wherefore the expected length of both the forward an reverse reads is 282 bp. 

Thus we have an overlap of `(282*2-348)/2` = 108 bp

We cut the reverse read to 240 bp, keeping an overlap of 66 bp. We do not trim the forward reads. 

The filtering parameters we’ll use are standard (however we allow for more expected errors as we only cut few bases and DADA is [robust to low quality sequences](https://twitter.com/bejcal/status/771010634074820608) )

+ maxN=0 (DADA2 requires no Ns)
+ truncQ=2 
+ maxEE=c(2,4)

The `maxEE` parameter sets the maximum number of “expected errors” allowed in a read. We allow for more expected errors in the reverse read as teh quality is slightly lower.

We use the `fastqPairedFilter` function to jointly filter the forward and reverse reads.


```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(filterpath, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filterpath, paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


```{r}

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(280,240),
                     maxN=0,
                     maxEE=c(2,4),
                     truncQ=2,
                     rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE) 

head(out)
```

```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
```


```{r, cache=TRUE}
plotErrors(errF, nominalQ=TRUE)
ggsave(paste(plotpath, "errorPlot_F.pdf", sep = "/"))

plotErrors(errR, nominalQ=TRUE)
ggsave(paste(plotpath, "errorPlot_R.pdf", sep = "/"))
```

```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```

```{r}
dadaFs[1]
```

Merge paired reads
Spurious sequence variants are further reduced by merging overlapping reads. 

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

head(mergers[[1]])
```

Construct sequence table:

```{r, "Seqeunce table"}

seqtab <- makeSequenceTable(mergers)
dim(seqtab)

data.frame(table(nchar(colnames(seqtab)))) %>% 
  ggplot( aes(x = as.numeric(as.character(Var1)), y = Freq))+
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = -90))+
  labs(x = "length in bp", title = "histogramm of merged sequence lenghts")+
  xlim(320, 420)

```

We can see that the sequences of the expected length (~348 bp) dominate the length distribution - which is good. But there is another cluster at ~350 bp, we keep all sequences for further exploration.


remove Chimeras

```{r, "Chimera removal", cache=TRUE}

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

```

percentage of Chimeric sequences: `r (1-(sum(seqtab.nochim)/sum(seqtab)))*100`

percentage of Chimeric ASVs: `r (1-(ncol(seqtab.nochim)/ncol(seqtab)))*100`

```{r}
write.table(seqtab.nochim, here("Data", "16_ASV_table.text"))
saveRDS(seqtab.nochim, here("Data", "16_ASV_table.RDS"))
#seqtab.nochim <- readRDS(here("Data", "seqtab.nochim_rl.RDS"))

seqtab.nochim <- readRDS(here("Data", "16_ASV_table.RDS"))
```


## track reads

how much reads lost during cutadapt?

```{r}
raw_seq <- list.files(path, full.names = TRUE)
raw_seq <- raw_seq[grepl("R1", raw_seq)]

raw_reads <- sapply(raw_seq, function(x) system(paste("zcat < ", x , "| echo $((`wc -l`/4))", sep = ""), intern = TRUE))

cutadapt <- 
  tibble(sample = sample.names, 
         raw = as.numeric(raw_reads))
```



```{r}
# summary of read numbers through DADA2 workflow
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names



track %>% 
  as.data.frame %>% 
  tibble::rownames_to_column(var="sample") %>% 
  left_join(cutadapt, .) %>% 
  mutate_at(vars(-sample), funs(prct = (./raw)*100)) %>% 
  select(sample,contains("prct")) %>% 
  gather(step, reads, -sample) %>% 
  mutate(step = factor(step, levels = c("raw_prct", 
                                        "input_prct",
                                        "filtered_prct", 
                                        "denoised_prct", "merged_prct", 
                                        "tabled_prct", "nonchim_prct"))) %>% 
  ggplot(aes(x=step, y=reads, group = sample))+
  geom_line(position = position_jitter(width = 0.1), colour = "grey", alpha = 0.3)+
  geom_point(position = position_jitter(width = 0.1), colour = "black",
             alpha = 0.3, shape = 21)+
  geom_violin(fill = NA, aes(group = step))+
  stat_summary(geom="line", fun.y="median", aes(group = 1))+
  scale_y_continuous(breaks = seq(0,100,10), limits = c(0,102))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = -30, hjust = 0))

ggsave(paste(plotpath, "sequence_filtering.pdf", sep = "/"))
```

export ASVs to Fasta file 

```{r}

seqtab_fasta <- DNAStringSet(colnames(seqtab.nochim))

names(seqtab_fasta) <- paste(
  paste("seq", formatC(1:length(seqtab_fasta),
                       flag = "0",width = nchar(length(seqtab_fasta))),
        sep = "_"))

writeXStringSet(seqtab_fasta, here("Data", "16S_ASVs.fasta"), format = "fasta")

colnames(seqtab.nochim) <- names(seqtab_fasta)
```

export ASV table

```{r}
#write.table(seqtab.nochim, here("Data", "16S_ASV_table.text"))
#seqtab.nochim <- read.table(here("Data", "16S_ASV_table.text"))
```


Vsearch clustering

test clustering threshold
```{r}

RES <- tibble(ID = numeric(),
       n_cluster = numeric())

for (i in seq(0.9, 1, 0.005)){
  
  system(paste(
    "/Applications/vsearch/bin/vsearch --cluster_fast ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/16S_ASVs_nc.fa --centroids ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/test.fasta --id ", i))
  
  L <- system("grep -c '^>' ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/test.fasta", intern = T) 
  
  RES_temp <- tibble(ID = i,
                     n_cluster = as.numeric(L))
  
  RES <- rbind(RES, RES_temp)
  
}

ggplot(RES, aes(x = ID, y = n_cluster))+
  geom_point()

```
We will cluster the ASVs at 99% similarity 

```{r}
system(
    "/Applications/vsearch/bin/vsearch --cluster_fast ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/16S_ASVs_nc.fa --centroids ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/16S_clustered_99.fasta --sizein --sizeout --id 0.99 --uc ~/Documents/01_Work/01_Research/15_eDNA_pilot/Data/16S_99_cluster.txt")
```

The COI_99_cluster.txt file tells us which sequences have been clustered

column 09 gives the original sequence name
column 10 gives the centroid sequence to which it has been clustered
column 01 tells us if the sequence is itself a centroid sequence ("S") or clustered to a centroid ("H")
column 04 gives the % identity with the centroid (here between 100% and 99%)

```{r}

Cluster_ID <- read_tsv(here("Data", "16S_99_cluster.txt"), col_names = FALSE)

#filter only sequences that have been clusterd
Cluster_ID <- 
Cluster_ID %>% filter(X1 != "C") %>% 
  dplyr::rename(seq = X9, Type = X1, match = X10, pident = X4) %>% 
  select(seq, Type, match, pident) %>% 
  filter(Type == "H" ) %>% 
  arrange(match)

#strip abundance info from seqnames
Cluster_ID <- 
Cluster_ID %>% 
  mutate(seq = gsub("(.?);size.+", "\\1", seq)) %>% 
  mutate(match = gsub("(.?);size.+", "\\1", match))

#split by cluster centroid
Cluster_list <- split(Cluster_ID, Cluster_ID$match)


#sum abundance of clustered sequences
seqtab.nochim_list <- 
  lapply(Cluster_list, function(x) {
  S_seq <- unique(x$match)
  C_seqs <- x$seq
  seqtab.nochim[,S_seq] <- rowSums(seqtab.nochim[,c(S_seq,C_seqs)])
})

#create new ASV table with only the centroid sequences
seqtab.nochim_C <- seqtab.nochim
seqtab.nochim_C[,names(seqtab.nochim_list)] <- t(bind_rows(seqtab.nochim_list))
seqtab.nochim_C <- seqtab.nochim_C[, which(! colnames(seqtab.nochim_C) %in% Cluster_ID$seq)] 

#remove chimeric sequences
seqtab.nochim_C <- seqtab.nochim_C[, !colnames(seqtab.nochim_C) %in%  gsub("(.?);size.+", "\\1", chim_seq)]

#export ASV table
write.table(seqtab.nochim_C, here("Data", "16S_ASV_table_99.text"))
seqtab.nochim_C <- read.table(here::here("Data", "16S_ASV_table_99.text"))


#export centroid fasta file
seqtab_fasta_C <- seqtab_fasta[colnames(seqtab.nochim_C)]
writeXStringSet(seqtab_fasta_C, here("Data", "16S_centroids_99.fasta"))
```

We assign the reads against a custom database created with Obitools, using ecoprc with our primers against the full EMBL nt database 


# Decipher

```{r}
seqtab_fasta_C <- readDNAStringSet(here("Data", "16S_centroids_99.fasta"))
load(here("Data" , "16S_trainingset_idtaxa.RData"))

TaxID_16S <- IdTaxa(test = seqtab_fasta_C,
                    trainingSet = trainingSet,
                    type = "collapsed",
                    threshold = 40,
                    strand = "both",
                    processors = 4)

saveRDS(TaxID_16S, here("Data", "TaxID_16S.RDS"))
TaxID_16S <- readRDS(here("Data", "TaxID_16S.RDS"))
```

```{r}
TaxID_16S_list <- str_split(TaxID_16S, ";")
names(TaxID_16S_list) <- attr(TaxID_16S,"names")

TaxID_16S_df <- 
  lapply(TaxID_16S_list, function(x) {as_tibble(t(x))}) %>% 
  bind_rows(.id = "seq")

TaxID_16S_df <- 
TaxID_16S_df %>% 
  select(-V1, -V2) %>% 
  dplyr::rename(Phylum = V3,
                 Class = V4,
                 Order = V5,
                 Family = V6,
                 Genus = V7,
                 Species = V8)

TaxID_16S_prob <- 
  TaxID_16S_df %>% 
  mutate(across(!one_of("seq"), function(x) as.numeric(
    gsub(".+\\[(\\d+\\.\\d)%\\]", "\\1", x))))

TaxID_16S_df <- 
  TaxID_16S_df %>% 
  mutate(across(!one_of("seq"), function(x) {
    gsub("(.+)\\[.+", "\\1", x) %>% 
    str_trim}))

write_tsv(TaxID_16S_df, here("Data", "TaxID_16S.txt"))
write_tsv(TaxID_16S_prob, here("Data", "TaxID_16S_prob.txt"))

```


#Sintax

+ sintax formating

example 
>AB008314;tax=d:Bacteria,p:Firmicutes,c:Bacilli,o:Lactobacillales,
  f:Streptococcaceae,g:Streptococcus;

```{r}
ref_16S <- readDNAStringSet(here("Data/16S_refDB_99_clean.fasta"))

sintax_names <- 
names(ref_16S) %>% 
  strsplit(";") %>% 
  lapply(function(x) {paste(c("", "tax=p:", "c:", "o:", "f:", "g:", "s:"),
                            x[c(1,3:length(x))], sep = "")}) %>% 
  lapply(function(x) paste(x[1], paste0(x[2:length(x)], collapse = ","), sep = ";")) %>% 
  unlist

names(ref_16S) <- sintax_names

writeXStringSet(ref_16S, here("Data", "16S_SINTAX.fasta"))
```

```{bash}

/Applications/vsearch/bin/vsearch --sintax /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/16S_centroids_99.fasta --db /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/16S_SINTAX.fasta --sintax_cutoff 0.1 --tabbedout /Users/fabian/Documents/01_Work/01_Research/15_eDNA_pilot/Data/16S_SINTAX_taxonomy.txt

```


```{r}
Sintax_taxa <- read_lines(here("Data", "16S_SINTAX_taxonomy.txt"))

#get table with all assignments (even the unlikely ones)
Sintax_taxa_table_full <- 
tibble(seq = gsub("(seq_\\d+).+", "\\1",Sintax_taxa),
       taxonomy = gsub("seq_\\d+\\\tp:(.+)\\\t\\+\\\tp:.+", "\\1", Sintax_taxa)) %>% 
  mutate(taxonomy = case_when(grepl("seq", taxonomy, fixed = T) ~ NA_character_,
                              TRUE ~ taxonomy)) %>% 
  separate(taxonomy, into = c("Phylum", "Class", "Order", "Family", "Genus", "Species"), sep = ",\\w:") %>% 
  mutate(across(!one_of("seq"), function(x) gsub("(\\w+)\\(.+\\)", "\\1", x))) %>% 
  mutate(across(!one_of("seq"), function(x) ifelse(x == "NA", NA_character_, x))) %>% 
  arrange(seq)

#get table with probabilities for each assignment
Sintax_taxa_table_prob <- 
  tibble(seq = gsub("(seq_\\d+).+", "\\1",Sintax_taxa),
       taxonomy = gsub("seq_\\d+\\\tp:(.+)\\\t\\+\\\tp:.+", "\\1", Sintax_taxa)) %>% 
  mutate(taxonomy = case_when(grepl("seq", taxonomy, fixed = T) ~ NA_character_,
                              TRUE ~ taxonomy)) %>% 
  separate(taxonomy, into = c("Phylum", "Class", "Order", "Family", "Genus", "Species"), sep = ",\\w:") %>% 
  mutate(across(!one_of("seq"), function(x) gsub("\\w+\\((.+)\\)", "\\1", x))) %>% 
  arrange(seq) %>% 
  mutate(across(2:7, as.numeric))


```

##rgbif

harmonizing taxonomy with gbif backbone for comparability with 16S data

```{r}

plan(multisession)

gbif_specnames <- 
future_map(1:nrow(Sintax_taxa_table_full), function(x) {
  name_backbone(name = Sintax_taxa_table_full$Species[x], 
                rank = "species", 
                kingdom = Sintax_taxa_table_full$Kingdom[x],
                phylum = Sintax_taxa_table_full$Phylum[x],
                class = Sintax_taxa_table_full$Class[x],
                order = Sintax_taxa_table_full$Order[x],
                family = Sintax_taxa_table_full$Family[x])
},
.progress = TRUE) %>% bind_rows()


Sintax_taxa_table_full <-  
  gbif_specnames %>% 
  select(kingdom, phylum, class, order, family, genus, species) %>% 
  mutate(seq = Sintax_taxa_table_full$seq) %>% 
  relocate(seq)

```


export

```{r}
  write_tsv(Sintax_taxa_table_full, here("Data", "16S_99_Sintax_tax.txt"))
write_tsv(Sintax_taxa_table_prob, here("Data", "16S_99_Sintax_prob.txt"))
```


#Tree

#phylogeny 

To complement the erroneous sequence filtering above, we build a phylogeny to detect aberrant sequences. For this we Align the Sequences with Decipher

```{r}
seq16S <- readDNAStringSet(here("Data", "16S_centroids_99.fasta"))
msa <- AlignSeqs(seq16S, processors = 4)  
```
We also follow [the advice](https://www.bioconductor.org/packages/devel/bioc/vignettes/DECIPHER/inst/doc/ArtOfAlignmentInR.pdf) to Stagger the alignment for more accurate Tree building

>To mitigate the problem of false homologies, StaggerAlignment will automatically generate a staggered version of an existing alignment. Staggered alignments separate potentially non-homologous regions into separate columns of the alignment. The result is an alignment that is less visually appealing, but likely more accurate in a phylogenetic sense. As such, this is an important post-processing step whenever the alignment will be used to construct a phylogenetic tree


```{r}
msa_stag <- StaggerAlignment(msa, verbose = FALSE)

distmsa <- DistanceMatrix(msa_stag)
```


export alignment
```{r}

writeXStringSet(msa_stag, file= here::here("Data", "16S_aligned.fasta"))

```

outlier tips
```{r}
Sintax_taxa_table_full <- read_tsv(here("Data", "16S_99_Sintax_tax.txt"))
Sintax_taxa_table_prob <- read_tsv(here("Data", "16S_99_Sintax_prob.txt"))

sintax_06 <- Sintax_taxa_table_full %>% 
  select(-kingdom)
sintax_06[Sintax_taxa_table_prob < 0.6] <- NA

distSum <- 
as.matrix(distmsa) %>% 
  rowSums() %>% 
  data.frame(dist = .) %>% 
  tibble::rownames_to_column(var = "seq") %>% 
  arrange(desc(dist)) %>% 
  mutate(rank = 1:n()) %>% 
  left_join(sintax_06) 

ggplot(distSum, aes(x = dist, fill = phylum))+
  geom_histogram()
```




We will use [FastTree](http://meta.microbesonline.org/fasttree/) to estimate an approximately-maximum-likelihood phylogenetic tree. It is orders of magnitudes faster than the corresponding R implementation phangorn. Also, because the downstream estimation of phylogenetic diversity requires an ultrametric tree, we use [PATHd8](http://www2.math.su.se/PATHd8/) for the phylogenetic dating. 

note that PATH8 produces a big file with lots of additional information that is not very relevant in our case. Therefore we grep the tree from the produced output file and save it separately. 

```{r}
system(paste("/Applications/FastTree -gtr -nt",
              here::here("Data", "16S_aligned.fasta"),
              ">",
              here::here("Data", "16S_FastTree.tre")))

TREE <- read.tree(here::here("Data", "16S_FastTree.tre"))
```

#plot tree 


```{r}

p_TREE <- ggtree(TREE)

p1 <- 
p_TREE+
  #geom_tiplab(align =TRUE, linesize = 0)+
  geom_facet(panel = "class", data = Sintax_taxa_table_full, geom = geom_segment, 
               mapping=aes(x = 1, xend = 3, yend = y, colour = phylum, group = 1))+
  scale_colour_brewer(palette = "Set1")+
  geom_facet(panel = "probability", data = Sintax_taxa_table_prob,  geom = geom_tile,
               mapping=aes(x = 1, fill = Phylum))+
  scale_fill_viridis_c()+
  theme(legend.position = "right")

ggsave(here("Figures",  "16S_Tree_annotated.pdf"), p1,  width = 6, height = 6)

plot(p1)
```
