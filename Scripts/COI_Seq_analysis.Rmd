---
title: "airborne eDNA - COI"
author: "Fabian Roger"
date: "2/4/2020"
output: html_document
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```


```{r, message = FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(here)
library(ggplot2)
library(dada2)
library(ShortRead)
library(rgbif)
```

This script follows the [DADA2 Pipeline Tutorial (1.18)](https://benjjneb.github.io/dada2/tutorial_1_8.html) 
The original article can be found [here](http://rdcu.be/ipGh)

The script was written with DADA2 version 1.18.0


```{r}
data.frame(`Sequencing centre` = "NGI",
           Project = "P14711, Y.Clough_19_01",  
           `data delivery` = "2020-01-31",
           Samples = "64 (2 * 32)",
           `Sequencing amount` = "?",
           `Sequencing type`  = "1 lane on MiSeq 2x300 bp (V3)",
           `Amplicon type` = "COI (BF3-BR2)") %>% 
  pivot_longer(everything())
```

```{r}
#COI
data.frame(`.` = c( "sequence","direction","length"),
           BF3 = c( "CCHGAYATRGCHTTYCCHCG", "foward", nchar("CCHGAYATRGCHTTYCCHCG")),
           BR2 = c( "TCDGGRTGNCCRAARAAYCA", "reverse", nchar("TCDGGRTGNCCRAARAAYCA"))) 

```

load sample information
```{r}
sampleInfo <- read_tsv(url("https://figshare.com/ndownloader/files/30448434"))
```


create new folder for COI sequencing analysis
create new folder for raw fastq sequencing data

```{r, eval = FALSE}
dir.create(here("Data", "Sequencing_data", "COI"))
dir.create(here("Data", "Sequencing_data", "COI", "raw"))
```

find all fastq files in raw data from NGI (should be 2*64 = 128)
copy all COI fastQ files to COI/raw

```{r, eval = FALSE}
#list all files including in subfolder, give full path back
files <- list.files(here("Data", "Sequencing_data", "P14711"),
           full.names = TRUE, recursive = TRUE)

#grep for compressed fastq files
files_fastq <- files[grep(".+\\.fastq\\.gz", files)]

#check if we find the expected number of files
length(files_fastq) == 128

#names of COI samples
COI_samples <- 
sampleInfo %>% 
  filter(Barcode == "COI") %>% 
  pull(Sample)

files_COI <- files_fastq[grep(paste0(COI_samples, collapse = "|"), files_fastq)]

length(files_COI) == 64

file.copy(files_COI, here("Data", "Sequencing_data", "COI", "raw"))
```

count number of reads in raw files
```{r}
files_fastq_fwrd <- files_fastq[grepl("_R1_", files_fastq)]
names(files_fastq_fwrd) <- gsub(".+?([A-Z,0-9]+_[0-9]+).+", "\\1", files_fastq_fwrd)

read_number <- 
lapply(files_fastq_fwrd, function(x) {
  
  system(paste("gunzip -c ", x, " | wc -l", sep = ""), intern = TRUE)
  
})

read_number <- 
read_number %>% 
  bind_rows() %>% 
  pivot_longer(everything(), names_to = "Sample", values_to = "reads") %>%
  mutate(reads = as.numeric(reads)/4) %>% 
  left_join(sampleInfo)

#total reads
read_number$reads %>% sum

#total reads by marker
read_number %>% 
  group_by(Barcode) %>% 
  summarise(reads = sum(reads))

#median, min and max
read_number %>% 
  summarise(min = min(reads),
            max = max(reads),
            median = median(reads))
```


define path variable that points to extracted, unmerged, sequencing reads

```{r, "path"}
path <- here("Data", "Sequencing_data", "COI", "raw")

# extract parent path
ParentPath <- here("Data", "Sequencing_data", "COI")

#path to plots
plotpath <- here("Data", "Sequencing_data", "COI", "plots")

#path to trimmed reads
trimmedpath <- here("Data", "Sequencing_data", "COI", "trimmed")

# make directory for trimmed files
dir.create( path =  trimmedpath)

# make directory for plots
dir.create( path =  plotpath)

#path to filtered reads
filterpath <- here("Data", "Sequencing_data", "COI", "DADA_filtered")

# make directory for filtered files
dir.create( path =  filterpath)

# list files
fns <- list.files(path)

data.frame(forward = fns[grepl("R1", fns)],
           reverse = fns[grepl("R2", fns)]) 
  
```

#remove primers
we check if the sequences still have primers / adapter sequences attached that need to be removed
For that we read in the first file and search for the primer sequence (and it's reverse complement) 

```{r}
# read in first sample. These are foward reads
Sample1F <- readFastq(paste(path, fns[1], sep = "/"))
Sample1R <- readFastq(paste(path, fns[2], sep = "/"))

# have a look at first 6 Sequences. 
sread(head(Sample1F))

# check for foward primer in first 10 000 sequences
# COI foward primer ["CCHGAYATRGCHTTYCCHCG"]

FPM <- vmatchPattern("CCHGAYATRGCHTTYCCHCG", sread(Sample1F)[1:10000], fixed=FALSE) 

start(FPM) %>% unlist %>% table
width(FPM) %>% unlist %>% table


# check for reverse primer in first 10 000 Seqs
# COI foward primer ["TCDGGRTGNCCRAARAAYCA"]
RPM <- vmatchPattern("TCDGGRTGNCCRAARAAYCA", sread(Sample1R)[1:10000], fixed=FALSE) 

start(RPM) %>% unlist %>% table
width(RPM) %>% unlist %>% table





```

We can see that both forward and reverse primers are still on (which makes sense as the reads are 301 bp long and thus must include primers). We need to remove them before we proceed.

We will use [cutadapt](https://cutadapt.readthedocs.io/en/stable/installation.html) to remove the forward and reverse primers from the paired reads.

To install cutadapt follow the installation instruction in the link. (you might need to install it in a virtual environment on macs)

cutadapt is executed from the command line but we can call it from within r

note: here I installed cutadapt in a virtual environment called 'my_env' and which is located under `/Users/fabian/my_env`
I first need to activate the virtual environment then I can use cutadapt. 

If you get a `command not found` error you either don't have cutadapt installed or it's not found (you need to activate your virtenv or put cutadapt in your PATH)

We trim the primers following the instructions [here](https://cutadapt.readthedocs.io/en/stable/recipes.html#trimming-amplicon-primers-from-both-ends-of-paired-end-reads)

Forward primer:

```{r}
Fwrd <- "CCHGAYATRGCHTTYCCHCG"
```

reverse complement Forward primer:

```{r}
RC_Fwrd <- reverseComplement(DNAString(Fwrd))
RC_Fwrd
```

Reverse Primer:

```{r}
Rev <- "TCDGGRTGNCCRAARAAYCA"
```

reverse complement Reverse Primer

```{r}
RC_Rev <- reverseComplement(DNAString(Rev))
RC_Rev
```


```{r}
forward_raw <- fns[grepl("R1", fns)] 
reverse_raw <- fns[grepl("R2", fns)]

#minimum length for sequences after trimming (shorter sequences are discarded)
minlength <- 100

#number of cores to use 
Cores <- 4


for (i in seq_along(forward_raw)) {

temp <- 
system(
paste("source /Users/fabian/my_env/bin/activate
       cutadapt -a ", Fwrd, "...", RC_Rev,
      " -A ", Rev, "...", RC_Fwrd,
      " --discard-untrimmed",
      " --minimum-length=", minlength, #minimum length for sequences after trimming
      " --cores=", Cores, # Number of cores to use
      " -o ", paste(trimmedpath, "/trimmed_", forward_raw[i], sep = ""),
      " -p ", paste(trimmedpath, "/trimmed_", reverse_raw[i], sep = ""),
      paste(" ", path, "/", forward_raw[i], sep = ""),
      paste(" ", path, "/", reverse_raw[i], sep = ""),
      sep = ""
      ),

intern = TRUE
)

rm(temp)
}


```

Now we read in the file names for all the trimmed fastq files and do a little string manipulation to get lists of the forward and reverse fastq files in matched order:

```{r, "sort fwrd and rev", cache=TRUE}

fnFs <- sort(list.files(trimmedpath, pattern="_R1_001.fastq.gz", full.names = TRUE)) # Just the forward read files
fnRs <- sort(list.files(trimmedpath, pattern="_R2_001.fastq.gz", full.names = TRUE)) # Just the reverse read files

# Get sample names from the first part of the forward read filenames
sample.names <- sapply(fnFs, function(x) gsub(".+?([A-Z,0-9]+_[0-9]+).+", "\\1", x), USE.NAMES = FALSE)

```

#quality profiles

Visualize the quality profile of the forward reads

```{r, "plot quality of reads", cache=TRUE}
# plot one forward and one reverse read in the rapport
plotQualityProfile(fnFs[[1]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "foward", sep = " - " ))

plotQualityProfile(fnRs[[1]])+
  scale_y_continuous(limits = c(0,40))+
  scale_x_continuous(breaks = seq(0,300, 20))+
  geom_hline(yintercept = 30, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 20, colour = "grey", linetype = "dashed")+
  labs(title = paste(sample.names[1], "reverse", sep = " - "))

```
The quality of the sequences looks very good as the median Q-score stays over 30 for the whole expected length (280 bp, 300 bp - 20bp primers)

The reverse read looks slightly worth (which is expected) but still good. 

We used the **BF3** - **BR2** Primers (Elbrecht et al, 2019). For insects, the primer should give us a fragment of 418 bp length (without primers). 

Both primers are 20 bp long, wherefore the expected length of both the forward an reverse reads is 280 bp. 

Thus we have an overlap of `280*2-418` = 142 bp

We cut the reverse read to 260 bp, keeping an overlap of 122 bp. We do not trim the forward reads. 

```{r}
frag_len <- 418
fwrd_len <- 280
rev_len <- 280
fwrd_trim <- 280
rev_trim <- 260


ggplot(data = tibble(x = c(1,frag_len), y = c(1,5)), aes(x = x, y = y))+
  geom_blank()+
  geom_segment(x = 0, xend = frag_len, y = 2, yend = 2)+ #fragment
  geom_segment(x = 0, xend = fwrd_len, y = 3, yend = 3)+ #fwrd
  geom_segment(x = frag_len-rev_len, xend = frag_len, y = 3.5, yend = 3.5)+ #rev
  geom_segment(x = fwrd_trim, xend = fwrd_len, y = 3, yend = 3, 
               colour = "red", size = 2, alpha = 0.2)+ #fwrd trim
  geom_segment(x = frag_len-rev_trim, xend = frag_len-rev_len,
               y = 3.5, yend = 3.5, 
               colour = "red", size = 3, alpha = 0.2)+ #rev trim
  geom_segment(x = frag_len-rev_trim, xend = fwrd_trim, y = 2, 
               yend = 2, colour = "green", size= 3, alpha = 0.2)+ #overlap
  geom_text(x = ((frag_len-rev_trim) + fwrd_trim)/2, y = 2.25, 
            label = paste(as.character(fwrd_trim+rev_trim-frag_len), "bp"),
            colour = "blue", size= 5)+
  theme_void()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  scale_x_continuous(breaks = seq(0,frag_len, 12))
```

The filtering parameters we’ll use are standard (however we allow for more expected errors as we only cut few bases and DADA is [robust to low quality sequences](https://twitter.com/bejcal/status/771010634074820608) )

+ maxN=0 (DADA2 requires no Ns)
+ truncQ=25 
+ maxEE=c(2,4)

The `maxEE` parameter sets the maximum number of “expected errors” allowed in a read. We allow for more expected errors in the reverse read as the quality is slightly lower.

We use the `fastqPairedFilter` function to jointly filter the forward and reverse reads.

#filter and trim
```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(filterpath, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filterpath, paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


```{r}

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(280,260),
                     maxN=0,
                     maxEE=c(2,4),
                     truncQ=2,
                     rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE) 

head(out)
```

#learn errors
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
```


```{r, cache=TRUE}
plotErrors(errF, nominalQ=TRUE)
ggsave(paste(plotpath, "COI_errorPlot_F.pdf", sep = "/"))

plotErrors(errR, nominalQ=TRUE)
ggsave(paste(plotpath, "COI_errorPlot_R.pdf", sep = "/"))
```

#denoise
```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```

```{r}
dadaFs[1]
```

#merge
Merge paired reads
Spurious sequence variants are further reduced by merging overlapping reads. 

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

head(mergers[[1]])

```

Construct sequence table:

```{r, "Seqeunce table"}

seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```


#remove chimera

```{r, "Chimera removal", cache=TRUE}

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

```

percentage of Chimeric sequences: `r (1-(sum(seqtab.nochim)/sum(seqtab)))*100`

percentage of Chimeric ASVs: `r (1-(ncol(seqtab.nochim)/ncol(seqtab)))*100`


# export ASV table

```{r}
write_rds(seqtab.nochim, here("Data", "COI_ASV_table.rds"))
```

check sequence length

```{r}
data.frame(table(nchar(colnames(seqtab.nochim)))) %>% 
  ggplot( aes(x = Var1, y = Freq))+
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = -90))+
  labs(x = "length in bp", title = "histogramm of merged sequence lenghts")

#reads
tibble(seq_length = nchar(colnames(seqtab.nochim)),
           seq_abund = colSums(seqtab.nochim)) %>% 
  group_by(seq_length) %>% 
  summarise(len_abund = sum(seq_abund)) %>% 
  ungroup() %>% 
  mutate(exp = if_else(seq_length %in% c(412:424), "exp", "unexp")) %>% #allowing for some variation aroound teh expected read length of 418
  group_by(exp) %>% 
  summarise(len_abund = sum(len_abund)) %>% 
  mutate(prct = round(len_abund / sum(len_abund) * 100,2))
 

```

# length filtering

```{r}
seqtab.nochim_rlen <- seqtab.nochim[, nchar(colnames(seqtab.nochim)) %in% c(412:424)]
```


#track reads

how much reads lost during cutadapt?

```{r}
raw_seq <- list.files(path, full.names = TRUE)
raw_seq <- raw_seq[grepl("R1", raw_seq)]

raw_reads <- sapply(raw_seq, function(x) system(paste("zcat < ", x, " | echo $((`wc -l`/4))", sep = ""), intern = TRUE))

cutadapt <- 
  tibble(sample = sample.names, 
         raw = as.numeric(raw_reads))
```


```{r}
# summary of read numbers through DADA2 workflow
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names

track %>% 
  as.data.frame %>% 
  tibble::rownames_to_column(var="sample") %>% 
  mutate("len_filtered" = rowSums(seqtab.nochim_rlen)) %>% 
  left_join(cutadapt, .) %>% 
  mutate(across(!one_of("sample"),  function(x) x/raw*100)) %>% 
  pivot_longer(!one_of("sample"), names_to = "step", values_to = "reads") %>% 
  mutate(step = factor(step, levels = c("raw", "input", "filtered", 
                                        "denoised", "merged", 
                                        "tabled", "nonchim", "len_filtered"))) %>%
  left_join(select(sampleInfo, sample, Station)) %>% 
  ggplot(aes(x=step, y=reads, group = sample, colour = Station))+
  geom_line(position = position_jitter(width = 0.1), alpha = 0.8)+
  geom_point(position = position_jitter(width = 0.1),
             alpha = 0.3, shape = 21)+
  geom_violin(fill = NA, aes(group = step))+
  stat_summary(geom="line", fun.y="median", aes(group = 1))+
  scale_y_continuous(breaks = seq(0,100,10), limits = c(0,102))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = -30, hjust = 0))+
  scale_colour_brewer(palette = "Set1")

ggsave(paste(plotpath, "COI_sequence_filtering.pdf", sep = "/"))
```


export ASVs to Fasta file 

```{r}
seqs <- DNAStringSet(colnames(seqtab.nochim_rlen))

names(seqs) <- paste("seq", 
                             formatC(1:length(seqs), flag = "0",
                                     width = nchar(length(seqs))),
                             sep = "_")

writeXStringSet(seqs, here("Data", "COI_ASVs.fasta"), format = "fasta")

seqs <- readDNAStringSet(here("Data", "COI_ASVs.fasta"))
```


```{r}
colnames(seqtab.nochim_rlen) <- names(seqs)

write_rds(seqtab.nochim_rlen, here("Data", "COI_seqtab.nochim_rl.RDS"))
#seqtab.nochim_rlen <- read_rds(here("Data", "COI_seqtab.nochim_rl.RDS"))
```

#Vsearch clustering

We use Vsearch for clustering and taxonomic assignment. See here for installation instructions: 
https://github.com/torognes/vsearch

test clustering threshold
```{r}

RES_COI <- tibble(ID = numeric(),
                  n_cluster = numeric())

for (i in seq(0.9, 1, 0.005)){
  
  system(paste(
    "/Applications/vsearch/bin/vsearch --cluster_fast ", 
    here("Data", "COI_ASVs.fasta"),
    " --centroids ",
    here("Data", "test.fasta"),
    " --id ",
    i,
    sep = ""))
  
  L <- system(paste("grep -c '^>' ", 
                    here("Data", "test.fasta"),
                    sep = ""), intern = T) 
  
  RES_temp <- tibble(ID = i,
                     n_cluster = as.numeric(L))
  
  RES_COI <- rbind(RES_COI, RES_temp)
  
}

ggplot(RES_COI, aes(x = 100*ID, y = n_cluster))+
  geom_point()+
  theme_minimal()+
  labs(x = "clustering threshold (% similarity)", y = " number of clusters",
       title = "COI - OTU clusters for differnt clustering thresholds")

ggsave(here("Figures", "Cluster_thresh_COI.pdf"))

```


```{r}
RES_COI %>% 
  arrange(desc(ID)) %>% 
  mutate(prct = n_cluster/max(n_cluster)*100)
```


cluster AVSs to 99% similarity

```{r}
system(
  
  paste("/Applications/vsearch/bin/vsearch --cluster_fast ", 
        here("Data", "COI_ASVs.fasta"),
        " --centroids ",
        here("Data", "COI_clustered_99.fasta"),
        " --id 0.99 --uc ",
        here("Data", "COI_99_cluster.txt"),
        sep = "")
)
    

```

The COI_99_cluster.txt file tells us which sequences have been clustered

column 09 gives the original sequence name
column 10 gives the centroid sequence to which it has been clustered
column 01 tells us if the sequence is itself a centroid sequence ("S") or clustered to a centroid ("H")
column 04 gives the % identity with the centroid (here between 100% and 99%)

```{r}

Cluster_ID <- read_delim(here("Data", "COI_99_cluster.txt"), delim = "\t", col_names = FALSE)

#filter only sequences that have been clusterd
Cluster_ID <- 
Cluster_ID %>% 
  filter(X1 != "C") %>% 
  dplyr::rename(seq = X9, Type = X1, match = X10, pident = X4) %>% 
  select(seq, Type, match, pident) %>% 
  filter(Type == "H" ) %>% 
  arrange(match)

#split by cluster centroid
Cluster_list <- split(Cluster_ID, Cluster_ID$match)

#rename sequences in ASV table
colnames(seqtab.nochim_rlen) <- paste("seq", 
                             formatC(1:ncol(seqtab.nochim_rlen), flag = "0",
                                     width = nchar(ncol(seqtab.nochim_rlen))),
                             sep = "_")

#sum abundance of clustered sequences
seqtab.nochim_list <- 
  lapply(Cluster_list, function(x) {
  S_seq <- unique(x$match)
  C_seqs <- x$seq
  seqtab.nochim_rlen[,S_seq] <- rowSums(seqtab.nochim_rlen[,c(S_seq,C_seqs)])
})

#create new ASV table with only the centroid sequences
seqtab.nochim_C <- seqtab.nochim_rlen
seqtab.nochim_C[,names(seqtab.nochim_list)] <- t(bind_rows(seqtab.nochim_list))
seqtab.nochim_C <- seqtab.nochim_C[, which(! colnames(seqtab.nochim_C) %in% Cluster_ID$seq)] 

#export ASV table
write.table(seqtab.nochim_C, here("Data", "COI_ASV_table_99.text"))
#seqtab.nochim_C <- read.table(here::here("Data", "COI_ASV_table_99.text"))
```

#Sintax

The refernce database was downloaded from here:

http://www.reference-midori.info/download.php

We used the following version: 

Archive/GenBank242/SINTAX/uniq/MIDORI_UNIQ_GB242_CO1_SINTAX.fasta.gz

```{r}

system(
  paste(
    "/Applications/vsearch/bin/vsearch --sintax ",
    here("Data", "COI_clustered_99.fasta"),
    " --db ",
    here("MIDORI_UNIQ_SP_GB242_CO1_SINTAX.fasta"),
    " --sintax_cutoff 0.1 --tabbedout ",
    here("Data", "COI_midori_tax.txt"),
    sep = "")
  )

```

```{r}
Sintax_taxa <- read_lines(here("Data", "COI_midori_tax.txt"))

#get table with all assignments (even the unlikely ones)
Sintax_taxa_table_full <- 
tibble(seq = gsub("(seq_\\d+).+", "\\1",Sintax_taxa),
       taxonomy = gsub("seq_\\d+\\\tk:(.+)\\\t\\+\\\tk:.+", "\\1", Sintax_taxa)) %>% 
  mutate(taxonomy = case_when(grepl("\t", taxonomy, fixed = T) ~ NA_character_,
                              TRUE ~ taxonomy)) %>% 
  mutate(taxonomy = gsub("_.+?:", "\t", taxonomy)) %>%
  mutate(taxonomy = gsub("_\\d+.+$", "", taxonomy)) %>%
  separate(taxonomy, into = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"), sep = "\t") %>% 
  arrange(seq)

#get table with probabilities for each assignment
Sintax_taxa_table_prob <- 
  tibble(seq = gsub("(seq_\\d+).+", "\\1",Sintax_taxa),
       taxonomy = gsub("seq_\\d+\\\tk:(.+)\\\t\\+\\\tk:.+", "\\1", Sintax_taxa)) %>% 
  mutate(taxonomy = case_when(grepl("\t", taxonomy, fixed = T) ~ NA_character_,
                              TRUE ~ taxonomy)) %>% 
  mutate(taxonomy = gsub(".+?(\\(\\d\\.\\d\\d\\)).+?", "\\1", taxonomy)) %>%
  mutate(taxonomy = gsub("s:.+?(\\(\\d\\.\\d\\d\\))$", "\\1", taxonomy)) %>%
  mutate(taxonomy = gsub("\\)\\(", "\t", taxonomy)) %>% 
  mutate(taxonomy = gsub("\\)|\\(", "", taxonomy)) %>% 
  separate(taxonomy, into = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"), sep = "\t") %>% 
  arrange(seq) %>% 
  mutate(across(2:8, as.numeric))
```


##rgbif

harmonizing taxonomy with gbif backbone for comparability with 16S data

```{r}

plan(multisession)

gbif_specnames <- 
future_map(1:nrow(Sintax_taxa_table_full), function(x) {
  name_backbone(name = Sintax_taxa_table_full$Species[x], 
                rank = "species", 
                kingdom = Sintax_taxa_table_full$Kingdom[x],
                phylum = Sintax_taxa_table_full$Phylum[x],
                class = Sintax_taxa_table_full$Class[x],
                order = Sintax_taxa_table_full$Order[x],
                family = Sintax_taxa_table_full$Family[x])
},
.progress = TRUE) %>% bind_rows()


Sintax_taxa_table_full <-  
  gbif_specnames %>% 
  select(kingdom, phylum, class, order, family, genus, species) %>% 
  mutate(seq = Sintax_taxa_table_full$seq) %>% 
  relocate(seq)

```


export

```{r}
write_tsv(Sintax_taxa_table_full, here("Data", "COI_99_Sintax_tax.txt"))
write_tsv(Sintax_taxa_table_prob, here("Data", "COI_99_Sintax_prob.txt"))
```




